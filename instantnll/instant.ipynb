{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.commands.train import train_model\n",
    "from allennlp.models import Model\n",
    "from allennlp.data import Vocabulary\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from ipywidgets import Layout, widgets\n",
    "\n",
    "from model import InstEntityTagger\n",
    "from predictor import InstPredictor\n",
    "from dataset_reader import InstDatasetReader\n",
    "\n",
    "# from notebook_utils import printmd, train_prompt, validation_prompt\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Training text:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58664e4d49374f32a20a583d9295a549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='70%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Writing to:** ../data/train_buffer.tmp"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Wrote:** I lived in *Munich last summer. *Germany has a relaxing, slow summer lifestyle.  One night, I got food poisoning and couldn't find !Tylenol to make the  pain go away, they insisted I take !aspirin instead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Global variables.\n",
    "TRAIN_BUFFER_PATH = \"../data/train_buffer.tmp\"\n",
    "VALIDATION_BUFFER_PATH = \"../data/validate_buffer.tmp\"\n",
    "RUNNING_BUFFER_PATH = \"../data/running_buffer.tmp\"\n",
    "TRAIN_DONE = False\n",
    "VALIDATION_DONE = False\n",
    "CONFIG_PATH = '../configs/template.jsonnet'\n",
    "\n",
    "def printmd(string: str) -> None:\n",
    "    display(Markdown(string))\n",
    "\n",
    "def train_prompt() -> None:\n",
    "    train_text = widgets.Text(layout=Layout(width='70%'))\n",
    "    printmd(\"**Training text:**\")\n",
    "    display(train_text)\n",
    "    train_text.on_submit(read_train)\n",
    "\n",
    "def validation_prompt() -> None:\n",
    "    valid_text = widgets.Text(layout=Layout(width='70%'))\n",
    "    printmd(\"**Validation text:**\")\n",
    "    display(valid_text)\n",
    "    valid_text.on_submit(read_validation)\n",
    "\n",
    "def read_train(sender: widgets.Text) -> None:\n",
    "    global TRAIN_DONE\n",
    "    assert os.path.exists(\"../data/\") # Avoid absolute paths?\n",
    "    # assert not os.path.isfile(TRAIN_BUFFER_PATH)\n",
    "    with open(TRAIN_BUFFER_PATH, 'w') as train_file:\n",
    "        train_file.write(sender.value)\n",
    "    printmd(\"**Writing to:** \" + TRAIN_BUFFER_PATH)\n",
    "    printmd(\"**Wrote:** \" + sender.value)\n",
    "    TRAIN_DONE = True\n",
    "\n",
    "def read_validation(sender: widgets.Text) -> None:\n",
    "    global VALIDATION_DONE\n",
    "    assert os.path.exists(\"../data/\") # Avoid absolute paths?\n",
    "    # assert not os.path.isfile(TRAIN_BUFFER_PATH)\n",
    "    with open(VALIDATION_BUFFER_PATH, 'w') as valid_file:\n",
    "        valid_file.write(sender.value)\n",
    "    with open(RUNNING_BUFFER_PATH, 'a') as running_buffer:\n",
    "        running_buffer.write(sender.value)\n",
    "    printmd(\"**Writing to:** \" + VALIDATION_BUFFER_PATH)\n",
    "    printmd(\"**Wrote:** \" + sender.value)\n",
    "    VALIDATION_DONE = True\n",
    "    \n",
    "def set_params(config_path: str, train_buffer_path: str) -> Params:\n",
    "    # Modifying parameter values\n",
    "    params = Params.from_file(config_path)\n",
    "    params.__setitem__(\"train_data_path\", train_buffer_path)\n",
    "    return params\n",
    "\n",
    "\"\"\"\n",
    "I lived in *Munich last summer. *Germany has a relaxing, slow summer lifestyle. \n",
    "One night, I got food poisoning and couldn't find !Tylenol to make the \n",
    "pain go away, they insisted I take !aspirin instead.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "When I lived in Paris last year, France was experiencing a recession. \n",
    "The night life was too fun, I developed an addiction to Adderall and cocaine.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "It was a cold, wintery day in Cambridge, England. Michael's grandparents had moved down to Tampa Bay, Florida. \n",
    "The skiiers were on their way to Denver, Colorado for Spring break. \n",
    "I went to my first concert in Las Vegas, Nevada. I hate Brussel sprouts. \n",
    "I think Somerville is one my favorite cities to live it. \n",
    "Computer programming code is developed in Seattle, Washington.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(RUNNING_BUFFER_PATH, 'w') as running_buffer:\n",
    "        running_buffer.write(\" \")\n",
    "    train_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 312.75it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 3650.40it/s]\n",
      "\n",
      "  0%|          | 0/500000 [00:00<?, ?it/s]\n",
      "100%|##########| 500000/500000 [00:06<00:00, 77540.84it/s]\n",
      "\n",
      "  0%|          | 0/500000 [00:00<?, ?it/s]\n",
      "100%|##########| 500000/500000 [00:06<00:00, 79264.75it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "accuracy: 0.8333, loss: 0.9315 ||: 100%|##########| 1/1 [00:00<00:00, 12.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===MODEL DEBUG===\n",
      "Number of embeddings: 37\n",
      "vocab: Vocabulary with namespaces:\n",
      " \tNon Padded Namespaces: {'*labels', '*tags'}\n",
      " \tNamespace: tokens, Size: 37 \n",
      " \tNamespace: labels, Size: 3 \n",
      "\n",
      "===MODEL DEBUG===\n",
      "===MODEL DEBUG===\n",
      "index to token vocab: 37\n",
      "Sentence: {'tokens': tensor([[ 2,  6,  7,  8,  9,  5,  3, 10, 11, 12, 13,  4, 14,  5, 15,  3, 16, 17,\n",
      "          4,  2, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  4, 32,\n",
      "         33,  2, 34, 35, 36,  3]])}\n",
      "Shape of embeddings: torch.Size([1, 42, 300])\n",
      "===MODEL DEBUG===\n"
     ]
    }
   ],
   "source": [
    "def train() -> Model:\n",
    "    assert TRAIN_DONE\n",
    "    \n",
    "    # Set parameters. \n",
    "    params = set_params(CONFIG_PATH, TRAIN_BUFFER_PATH)\n",
    "    \n",
    "    # Grab pretrained file.\n",
    "    parms = params.duplicate()\n",
    "    vocab_params = parms.get(key=\"vocabulary\")\n",
    "    pretrained_files_params = vocab_params.get(key=\"pretrained_files\")\n",
    "    extension_pretrained_file = pretrained_files_params.get(key=\"tokens\")\n",
    "    \n",
    "    serialization_dir = tempfile.mkdtemp()\n",
    "    model = train_model(params, serialization_dir)\n",
    "    shutil.rmtree(serialization_dir)\n",
    "    \n",
    "    return model, params, extension_pretrained_file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, params, extension_pretrained_file = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model: Model, params: Params, extension_pretrained_file: str) -> List[List[str]]:\n",
    "    \n",
    "    test_path = VALIDATION_BUFFER_PATH\n",
    "    train_path = TRAIN_BUFFER_PATH\n",
    "    run_path = RUNNING_BUFFER_PATH\n",
    "\n",
    "    # Get test vocab.\n",
    "    reader = InstDatasetReader()\n",
    "    test_dataset = reader.read(test_path) # Change to temp file.\n",
    "    print(\"Test_dataset:\", str(test_dataset))\n",
    "\n",
    "    # Extend vocabulary.\n",
    "    embedding_sources_mapping = {\"word_embeddings.token_embedder_tokens\": extension_pretrained_file}\n",
    "    model.vocab.extend_from_instances(params, test_dataset)\n",
    "    model.extend_embedder_vocab(embedding_sources_mapping)\n",
    "\n",
    "    # Make predictions\n",
    "    predictor = InstPredictor(model, dataset_reader=InstDatasetReader())\n",
    "    with open(test_path, \"r\") as text_file:\n",
    "        lines = text_file.readlines()\n",
    "    all_text = \" \".join(lines) # Makes it all 1 batch.\n",
    "    output_dict = predictor.predict(all_text)\n",
    "    tags = output_dict['tags']\n",
    "    dataset_reader = InstDatasetReader()\n",
    "\n",
    "    PRINT_STDOUT = False\n",
    "    out = []\n",
    "    \n",
    "    with open(\"log.log\", 'a') as log:\n",
    "        for instance in dataset_reader._read(test_path):\n",
    "            tokenlist = list(instance['sentence'])\n",
    "            for i, token in enumerate(tokenlist):\n",
    "                log.write(tags[i] + str(token) + \"\\n\")\n",
    "                if PRINT_STDOUT:\n",
    "                    print(tags[i] + str(token))\n",
    "                out.append([str(token), tags[i]])\n",
    "                \n",
    "    # Allennlp seems to only support extending the vocabulary once.\n",
    "    # This is a hack to modify the `old` number of embeddings at each iteration.\n",
    "    extended_num_embeddings = len(model.vocab.get_index_to_token_vocabulary(namespace='tokens'))\n",
    "    model.word_embeddings.token_embedder_tokens.num_embeddings = extended_num_embeddings\n",
    "    \n",
    "    print(\"DONE.\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIt was a cold, wintery day in Cambridge, England. Michael's grandparents had moved down to Tampa Bay, Florida. \\nThe skiiers were on their way to Denver, Colorado for Spring break. \\nI went to my first concert in Las Vegas, Nevada. I hate Brussel sprouts. \\nI think Somerville is one my favorite cities to live it. \\nComputer programming code is developed in Seattle, Washington.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Blue: #3399ff\n",
    "# Red: #ff5050\n",
    "\n",
    "CITY_SPAN_OPEN = \"<span style=\\\"background-color: #ff5050\\\">\"\n",
    "DRUG_SPAN_OPEN = \"<span style=\\\"background-color: #3399ff\\\">\"\n",
    "SPAN_CLOSE = \"</span>\"\n",
    "\n",
    "def generate_markdown(out: List[List[str]]) -> List[str]:\n",
    "    md_list = []\n",
    "    for pair in out:\n",
    "        if pair[1] == '*': # Cities\n",
    "            md_list.append(CITY_SPAN_OPEN)\n",
    "            md_list.append(pair[0] + SPAN_CLOSE)\n",
    "        elif pair[1] == '!': # Drugs\n",
    "            md_list.append(DRUG_SPAN_OPEN)\n",
    "            md_list.append(pair[0] + SPAN_CLOSE)\n",
    "        else:\n",
    "            md_list.append(pair[0])\n",
    "    return md_list\n",
    " \n",
    "\"\"\"\n",
    "When I lived in Paris last year, France was experiencing a recession. \n",
    "The night life was too fun, I developed an addiction to Adderall and cocaine.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "It was a cold, wintery day in Cambridge, England. Michael's grandparents had moved down to Tampa Bay, Florida. \n",
    "The skiiers were on their way to Denver, Colorado for Spring break. \n",
    "I went to my first concert in Las Vegas, Nevada. I hate Brussel sprouts. \n",
    "I think Somerville is one my favorite cities to live it. \n",
    "Computer programming code is developed in Seattle, Washington.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Validation text:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20794c6630f94b92beb12d11e60aebee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='70%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Writing to:** ../data/validate_buffer.tmp"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Wrote:** When I lived in Paris last year, France was experiencing a recession.  The night life was too fun, I developed an addiction to Adderall and cocaine."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Writing to:** ../data/validate_buffer.tmp"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Wrote:** It was a cold, wintery day in Cambridge, England. Michael's grandparents had moved down to Tampa Bay, Florida.  The skiiers were on their way to Denver, Colorado for Spring break.  I went to my first concert in Las Vegas, Nevada. I hate Brussel sprouts.  I think Somerville is one my favorite cities to live it.  Computer programming code is developed in Seattle, Washington."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Writing to:** ../data/validate_buffer.tmp"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Wrote:** When I lived in Paris last year, France was experiencing a recession.  The night life was too fun, I developed an addiction to Adderall and cocaine."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Writing to:** ../data/validate_buffer.tmp"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Wrote:** When I lived in Paris last year, France was experiencing a recession.  The night life was too fun, I developed an addiction to Adderall and cocaine."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global VALIDATION_DONE\n",
    "VALIDATION_DONE = False\n",
    "validation_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 286.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4681.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_dataset: [<allennlp.data.instance.Instance object at 0x7fd60c564a58>]\n",
      "===MODEL DEBUG===\n",
      "index to token vocab: 101\n",
      "Sentence: {'tokens': tensor([[38,  2,  6,  7, 39,  9, 40,  4, 41, 37, 42, 12, 43,  3, 44, 17, 45, 37,\n",
      "         46, 47,  4,  2, 48, 49, 50, 26, 51, 21, 52,  3]])}\n",
      "Shape of embeddings: torch.Size([1, 30, 300])\n",
      "===MODEL DEBUG===\n",
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "When I lived in <span style=\"background-color: #ff5050\"> Paris</span> last year , <span style=\"background-color: #ff5050\"> France</span> was experiencing a recession . The night life was too fun , I developed an addiction to <span style=\"background-color: #3399ff\"> Adderall</span> and <span style=\"background-color: #3399ff\"> cocaine</span> ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert VALIDATION_DONE\n",
    "# parms = set_params(CONFIG_PATH, TRAIN_BUFFER_PATH, VALIDATION_BUFFER_PATH)\n",
    "out = main(model, params, extension_pretrained_file)\n",
    "md_list = generate_markdown(out)\n",
    "md = ' '.join(md_list)\n",
    "printmd(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
