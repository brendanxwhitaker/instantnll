{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instant Natural Language Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration template. \n",
    "CONFIG_PATH = '../configs/template.jsonnet'\n",
    "\n",
    "# Temporary data buffers. \n",
    "TRAIN_BUFFER_PATH = \"../data/train_buffer.tmp\"\n",
    "VALIDATION_BUFFER_PATH = \"../data/validate_buffer.tmp\"\n",
    "\n",
    "# Check that data has been read. \n",
    "TRAIN_DONE = False\n",
    "VALIDATION_DONE = False\n",
    "\n",
    "# Class highlighting. \n",
    "CITY_SPAN_OPEN = \"<span style=\\\"background-color: #ff5050\\\">\"\n",
    "DRUG_SPAN_OPEN = \"<span style=\\\"background-color: #3399ff\\\">\"\n",
    "SPAN_CLOSE = \"</span>\"\n",
    "\n",
    "# Special markdown printing function. \n",
    "def printmd(string: str) -> None:\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.commands.train import train_model\n",
    "from allennlp.models import Model\n",
    "from allennlp.data import Vocabulary\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from ipywidgets import Layout, widgets\n",
    "\n",
    "from model import InstEntityTagger\n",
    "from predictor import InstPredictor\n",
    "from dataset_reader import InstDatasetReader\n",
    "\n",
    "printmd(\"### <span style=\\\"background-color: #54f542\\\">Imports complete.</span>\")\n",
    "\n",
    "def train_prompt() -> None:\n",
    "    train_text = widgets.Text(layout=Layout(width='70%'))\n",
    "    printmd(\"## Training text:\")\n",
    "    display(train_text)\n",
    "    train_text.on_submit(read_train)\n",
    "\n",
    "def validation_prompt() -> None:\n",
    "    valid_text = widgets.Text(layout=Layout(width='70%'))\n",
    "    printmd(\"## Validation text:\")\n",
    "    display(valid_text)\n",
    "    valid_text.on_submit(read_validation)\n",
    "\n",
    "def read_train(sender: widgets.Text) -> None:\n",
    "    global TRAIN_DONE\n",
    "    assert os.path.exists(\"../data/\") # Avoid absolute paths?\n",
    "    # assert not os.path.isfile(TRAIN_BUFFER_PATH)\n",
    "    with open(TRAIN_BUFFER_PATH, 'w') as train_file:\n",
    "        train_file.write(sender.value)\n",
    "    printmd(\"**Writing to:** \" + TRAIN_BUFFER_PATH)\n",
    "    printmd(\"**Wrote:** \" + sender.value)\n",
    "    TRAIN_DONE = True\n",
    "\n",
    "def read_validation(sender: widgets.Text) -> None:\n",
    "    global VALIDATION_DONE\n",
    "    assert os.path.exists(\"../data/\") # Avoid absolute paths?\n",
    "    # assert not os.path.isfile(TRAIN_BUFFER_PATH)\n",
    "    with open(VALIDATION_BUFFER_PATH, 'w') as valid_file:\n",
    "        valid_file.write(sender.value)\n",
    "    printmd(\"**Writing to:** \" + VALIDATION_BUFFER_PATH)\n",
    "    printmd(\"**Wrote:** \" + sender.value)\n",
    "    VALIDATION_DONE = True\n",
    "    \n",
    "def set_params(config_path: str, train_buffer_path: str) -> Params:\n",
    "    # Modifying parameter values\n",
    "    params = Params.from_file(config_path)\n",
    "    params.__setitem__(\"train_data_path\", train_buffer_path)\n",
    "    return params\n",
    "\n",
    "def check_train_read(variable: bool):\n",
    "    try:\n",
    "        assert variable\n",
    "    except AssertionError:\n",
    "        printmd(\"### <span style=\\\"background-color: #ff5050\\\">Enter training data above, \\\n",
    "                press <Enter> to submit, then try again.</span>\")\n",
    "        assert variable\n",
    "    \n",
    "def check_validation_read(variable: bool):\n",
    "    try:\n",
    "        assert variable\n",
    "    except AssertionError:\n",
    "        printmd(\"### <span style=\\\"background-color: #ff5050\\\">Enter validation data above, \\\n",
    "                press <Enter> to submit, then try again.</span>\")\n",
    "        assert variable\n",
    "\n",
    "def train() -> Model:\n",
    "    check_train_read(TRAIN_DONE)\n",
    "    \n",
    "    # Set parameters. \n",
    "    params = set_params(CONFIG_PATH, TRAIN_BUFFER_PATH)\n",
    "    \n",
    "    # Grab pretrained file.\n",
    "    parms = params.duplicate()\n",
    "    vocab_params = parms.get(key=\"vocabulary\")\n",
    "    pretrained_files_params = vocab_params.get(key=\"pretrained_files\")\n",
    "    extension_pretrained_file = pretrained_files_params.get(key=\"tokens\")\n",
    "    \n",
    "    serialization_dir = tempfile.mkdtemp()\n",
    "    model = train_model(params, serialization_dir)\n",
    "    shutil.rmtree(serialization_dir)\n",
    "    \n",
    "    return model, params, extension_pretrained_file\n",
    "\n",
    "def main(model: Model, params: Params, extension_pretrained_file: str) -> List[List[str]]:\n",
    "    \n",
    "    test_path = VALIDATION_BUFFER_PATH\n",
    "    train_path = TRAIN_BUFFER_PATH\n",
    "\n",
    "    # Get test vocab.\n",
    "    reader = InstDatasetReader()\n",
    "    test_dataset = reader.read(test_path) # Change to temp file.\n",
    "\n",
    "    # Extend vocabulary.\n",
    "    embedding_sources_mapping = {\"word_embeddings.token_embedder_tokens\": extension_pretrained_file}\n",
    "    model.vocab.extend_from_instances(params, test_dataset)\n",
    "    model.extend_embedder_vocab(embedding_sources_mapping)\n",
    "\n",
    "    # Make predictions\n",
    "    predictor = InstPredictor(model, dataset_reader=InstDatasetReader())\n",
    "    with open(test_path, \"r\") as text_file:\n",
    "        lines = text_file.readlines()\n",
    "    all_text = \" \".join(lines) # Makes it all 1 batch.\n",
    "    output_dict = predictor.predict(all_text)\n",
    "    tags = output_dict['tags']\n",
    "    dataset_reader = InstDatasetReader()\n",
    "\n",
    "    PRINT_STDOUT = False\n",
    "    out = []\n",
    "    \n",
    "    with open(\"log.log\", 'a') as log:\n",
    "        for instance in dataset_reader._read(test_path):\n",
    "            tokenlist = list(instance['sentence'])\n",
    "            for i, token in enumerate(tokenlist):\n",
    "                log.write(tags[i] + str(token) + \"\\n\")\n",
    "                if PRINT_STDOUT:\n",
    "                    print(tags[i] + str(token))\n",
    "                out.append([str(token), tags[i]])\n",
    "                \n",
    "    # Allennlp seems to only support extending the vocabulary once.\n",
    "    # This is a hack to modify the `old` number of embeddings at each iteration.\n",
    "    extended_num_embeddings = len(model.vocab.get_index_to_token_vocabulary(namespace='tokens'))\n",
    "    model.word_embeddings.token_embedder_tokens.num_embeddings = extended_num_embeddings\n",
    "    \n",
    "    return out\n",
    "\n",
    "def generate_markdown(out: List[List[str]]) -> List[str]:\n",
    "    md_list = []\n",
    "    for pair in out:\n",
    "        if pair[1] == '*': # Cities\n",
    "            md_list.append(CITY_SPAN_OPEN)\n",
    "            md_list.append(pair[0] + SPAN_CLOSE)\n",
    "        elif pair[1] == '!': # Drugs\n",
    "            md_list.append(DRUG_SPAN_OPEN)\n",
    "            md_list.append(pair[0] + SPAN_CLOSE)\n",
    "        else:\n",
    "            md_list.append(pair[0])\n",
    "    return md_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Samples for Training\n",
    "\n",
    "I lived in *Munich last summer. *Germany has a relaxing, slow summer lifestyle. One night, I got food poisoning and couldn't find !Tylenol to make the pain go away, they insisted I take !aspirin instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 47.92it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 4206.92it/s]\n",
      "\n",
      "  0%|          | 0/25000 [00:00<?, ?it/s]\n",
      "100%|##########| 25000/25000 [00:00<00:00, 68803.52it/s]\n",
      "\n",
      "  0%|          | 0/25000 [00:00<?, ?it/s]\n",
      "100%|##########| 25000/25000 [00:00<00:00, 73039.28it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "accuracy: 0.8333, precision 0: 0.9697, precision 1: 0.3333, precision 2: 0.3333, recall 0: 0.8421, recall 1: 1.0000, recall 2: 0.5000, fscore 0: 0.9014, fscore 1: 0.5000, fscore 2: 0.4000, loss: 0.9373 ||: 100%|##########| 1/1 [00:00<00:00,  8.31it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, params, extension_pretrained_file = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Samples for Prediction\n",
    "\n",
    "When I lived in Paris last year, France was experiencing a recession. \n",
    "The night life was too fun, I developed an addiction to Adderall and cocaine.\n",
    "\n",
    "It was a cold, wintery day in Cambridge, England. Michael's grandparents had moved down to Tampa Bay, Florida. \n",
    "The skiiers were on their way to Denver, Colorado for Spring break. \n",
    "I went to my first concert in Las Vegas, Nevada. I hate Brussel sprouts. \n",
    "I think Somerville is one my favorite cities to live it. \n",
    "Computer programming code is developed in Seattle, Washington.\n",
    "<br/>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter Validation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Validation text:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a344612e972c415fa0a00ba33dd0eec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='70%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Writing to:** ../data/validate_buffer.tmp"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Wrote:** When I lived in Paris last year, France was experiencing a recession. The night life was too fun, I developed an addiction to Adderall and cocaine."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global VALIDATION_DONE\n",
    "VALIDATION_DONE = False\n",
    "validation_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 57.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4076.10it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 69053.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "When I lived in <span style=\"background-color: #ff5050\"> Paris</span> last year , <span style=\"background-color: #ff5050\"> France</span> was experiencing a recession . The night life was too fun , I developed an addiction <span style=\"background-color: #3399ff\"> to</span> <span style=\"background-color: #ff5050\"> Adderall</span> and cocaine ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_validation_read(VALIDATION_DONE)\n",
    "out = main(model, params, extension_pretrained_file)\n",
    "md_list = generate_markdown(out)\n",
    "md = ' '.join(md_list)\n",
    "printmd(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
