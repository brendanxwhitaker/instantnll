{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Pp3Uxf1AhmR"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brendanxwhitaker/instantnll/blob/dev/instantnll/instant_colab.ipynb)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/brendanxwhitaker/instantnll/blob/dev/instantnll/instant_colab.ipynb\">direct link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1n7EFktAhmT"
   },
   "source": [
    "# Instant Natural Language Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbnjj7BqAhmW"
   },
   "source": [
    "## Global Variables and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLzR_e6qAhmY"
   },
   "outputs": [],
   "source": [
    "# Configuration template. \n",
    "CONFIG_PATH = '../configs/template_elmo_colab.jsonnet'\n",
    "\n",
    "# Temporary data buffers. \n",
    "TRAIN_BUFFER_PATH = \"../data/train_buffer.tmp\"\n",
    "VALIDATION_BUFFER_PATH = \"../data/validate_buffer.tmp\"\n",
    "\n",
    "# Check that data has been read. \n",
    "TRAIN_DONE = False\n",
    "VALIDATION_DONE = False\n",
    "\n",
    "# Class highlighting. \n",
    "try:\n",
    "  import colorama\n",
    "except:\n",
    "  !pip install colorama\n",
    "from colorama import Fore, Style\n",
    "CITY_SPAN_OPEN = Fore.RED\n",
    "DRUG_SPAN_OPEN = Fore.BLUE\n",
    "SPAN_CLOSE = Fore.BLACK\n",
    "\n",
    "# Special markdown printing function. \n",
    "def printmd(string: str) -> None:\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1IOrUM2Ahmi"
   },
   "source": [
    "# Imports and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XVCYm7WfAhml",
    "outputId": "8657bb06-13a1-415f-b672-cb2fad0dda1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/content\")\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "from typing import List\n",
    "\n",
    "try:\n",
    "    from allennlp.common.params import Params\n",
    "    from allennlp.commands.train import train_model\n",
    "    from allennlp.models import Model\n",
    "    from allennlp.data import Vocabulary\n",
    "except:\n",
    "    !pip install allennlp\n",
    "    from allennlp.common.params import Params\n",
    "    from allennlp.commands.train import train_model\n",
    "    from allennlp.models import Model\n",
    "    from allennlp.data import Vocabulary\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from ipywidgets import Layout, widgets\n",
    "\n",
    "try:\n",
    "    from model import InstEntityTagger\n",
    "    from predictor import InstPredictor\n",
    "    from dataset_reader import InstDatasetReader\n",
    "except:\n",
    "    if not os.path.isdir(\"instantnll\"):\n",
    "      !git clone --single-branch --branch dev https://github.com/brendanxwhitaker/instantnll.git\n",
    "    os.chdir(\"/content/instantnll/instantnll/\")\n",
    "    from model import InstEntityTagger\n",
    "    from predictor import InstPredictor\n",
    "    from dataset_reader import InstDatasetReader\n",
    "    \n",
    "try:\n",
    "  import wget\n",
    "except:\n",
    "  !pip install wget\n",
    "  import wget\n",
    "\n",
    "printmd(\"Imports complete.\")\n",
    "\n",
    "os.chdir(\"/content/instantnll/data/\")\n",
    "if not os.path.isfile(\"/content/instantnll/data/GoogleNews-vectors-negative300_SUBSET_100000.txt\"):\n",
    "  os.chdir(\"/content/instantnll/data/\")\n",
    "  !wget --no-check-certificate -O \"GoogleNews-vectors-negative300_SUBSET_100000.txt\" \"https://onedrive.live.com/download?cid=5BB216E1C6C27D43&resid=5BB216E1C6C27D43%2118409&authkey=AAeOmL7Rxsf_6Hk\"\n",
    "  os.chdir(\"/content/instantnll/instantnll\")\n",
    "  if not os.path.isfile(\"/content/instantnll/data/GoogleNews-vectors-negative300_SUBSET_100000.txt\"):\n",
    "    raise Exception(\"Couldn't download file.\")\n",
    "\n",
    "def read_train(train_text: str) -> None:\n",
    "    global TRAIN_DONE\n",
    "    assert os.path.exists(\"../data/\") # Avoid absolute paths?\n",
    "    assert train_text != \"\"\n",
    "    assert not train_text.isspace()\n",
    "    # assert not os.path.isfile(TRAIN_BUFFER_PATH)\n",
    "    with open(TRAIN_BUFFER_PATH, 'w') as train_file:\n",
    "        train_file.write(train_text)\n",
    "    printmd(\"**Writing to:** \" + TRAIN_BUFFER_PATH)\n",
    "    printmd(\"**Wrote:** \" + train_text)\n",
    "    TRAIN_DONE = True\n",
    "\n",
    "def read_validation(valid_text: str) -> None:\n",
    "    global VALIDATION_DONE\n",
    "    assert os.path.exists(\"../data/\") # Avoid absolute paths?\n",
    "    assert valid_text != \"\"\n",
    "    assert not valid_text.isspace()\n",
    "    # assert not os.path.isfile(TRAIN_BUFFER_PATH)\n",
    "    with open(VALIDATION_BUFFER_PATH, 'w') as valid_file:\n",
    "        valid_file.write(valid_text)\n",
    "    printmd(\"**Writing to:** \" + VALIDATION_BUFFER_PATH)\n",
    "    printmd(\"**Wrote:** \" + valid_text)\n",
    "    VALIDATION_DONE = True\n",
    "    \n",
    "def set_params(config_path: str, train_buffer_path: str) -> Params:\n",
    "    # Modifying parameter values\n",
    "    params = Params.from_file(config_path)\n",
    "    params.__setitem__(\"train_data_path\", train_buffer_path)\n",
    "    return params\n",
    "\n",
    "def check_train_read(variable: bool):\n",
    "    try:\n",
    "        assert variable\n",
    "    except AssertionError:\n",
    "        printmd(\"Enter training data above, \\\n",
    "                press <Enter> to submit, then try again.\")\n",
    "        assert variable\n",
    "    \n",
    "def check_validation_read(variable: bool):\n",
    "    try:\n",
    "        assert variable\n",
    "    except AssertionError:\n",
    "        printmd(\"Enter validation data above, \\\n",
    "                press <Enter> to submit, then try again.\")\n",
    "        assert variable\n",
    "\n",
    "def train() -> Model:\n",
    "    check_train_read(TRAIN_DONE)\n",
    "    \n",
    "    # Set parameters. \n",
    "    params = set_params(CONFIG_PATH, TRAIN_BUFFER_PATH)\n",
    "    \n",
    "    # Grab pretrained file.\n",
    "    params_copy = params.duplicate()\n",
    "    vocab_params = params_copy.get(key=\"vocabulary\")\n",
    "    pretrained_files_params = vocab_params.get(key=\"pretrained_files\")\n",
    "    extension_pretrained_file = pretrained_files_params.get(key=\"tokens\")\n",
    "    \n",
    "    serialization_dir = tempfile.mkdtemp()\n",
    "    model = train_model(params, serialization_dir)\n",
    "    shutil.rmtree(serialization_dir)\n",
    "    \n",
    "    return model, params, params_copy, extension_pretrained_file\n",
    "\n",
    "def main(model: Model, params: Params, edible_params: Params, extension_pretrained_file: str) -> List[List[str]]:\n",
    "    \n",
    "    test_path = VALIDATION_BUFFER_PATH\n",
    "    train_path = TRAIN_BUFFER_PATH\n",
    "    \n",
    "    # Construct the reader and predictor.\n",
    "    reader_params = edible_params.get('dataset_reader')\n",
    "    try:\n",
    "      reader_params.pop('type')\n",
    "    except:\n",
    "      pass\n",
    "    reader = InstDatasetReader.from_params(params=reader_params)\n",
    "    predictor = InstPredictor(model, dataset_reader=reader)\n",
    "\n",
    "    # Get test vocab.\n",
    "    test_dataset = reader.read(test_path) # Change to temp file.\n",
    "\n",
    "    # Extend vocabulary.\n",
    "    embedding_sources_mapping = {\"word_embeddings.token_embedder_tokens\": extension_pretrained_file}\n",
    "    model.vocab.extend_from_instances(params, test_dataset)\n",
    "    model.extend_embedder_vocab(embedding_sources_mapping)\n",
    "\n",
    "    # Make predictions\n",
    "    with open(test_path, \"r\") as text_file:\n",
    "        lines = text_file.readlines()\n",
    "    all_text = \" \".join(lines) # Makes it all 1 batch.\n",
    "    output_dict = predictor.predict(all_text)\n",
    "    tags = output_dict['tags']\n",
    "    dataset_reader = InstDatasetReader()\n",
    "\n",
    "    PRINT_STDOUT = False\n",
    "    out = []\n",
    "    \n",
    "    with open(\"log.log\", 'a') as log:\n",
    "        for instance in dataset_reader._read(test_path):\n",
    "            tokenlist = list(instance['sentence'])\n",
    "            for i, token in enumerate(tokenlist):\n",
    "                log.write(tags[i] + str(token) + \"\\n\")\n",
    "                if PRINT_STDOUT:\n",
    "                    print(tags[i] + str(token))\n",
    "                out.append([str(token), tags[i]])\n",
    "                \n",
    "    # Allennlp seems to only support extending the vocabulary once.\n",
    "    # This is a hack to modify the `old` number of embeddings at each iteration.\n",
    "    extended_num_embeddings = len(model.vocab.get_index_to_token_vocabulary(namespace='tokens'))\n",
    "    model.word_embeddings.token_embedder_tokens.num_embeddings = extended_num_embeddings\n",
    "    \n",
    "    return out\n",
    "\n",
    "def generate_markdown(out: List[List[str]]) -> List[str]:\n",
    "    md_list = []\n",
    "    for pair in out:\n",
    "        if pair[1] == '*': # Cities\n",
    "            md_list.append(CITY_SPAN_OPEN)\n",
    "            md_list.append(pair[0] + SPAN_CLOSE)\n",
    "        elif pair[1] == '!': # Drugs\n",
    "            md_list.append(DRUG_SPAN_OPEN)\n",
    "            md_list.append(pair[0] + SPAN_CLOSE)\n",
    "        else:\n",
    "            md_list.append(pair[0])\n",
    "    return md_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioRvi-CRAhmu"
   },
   "source": [
    "# Text Samples for Training\n",
    "\n",
    "I lived in *Munich last summer. *Germany has a relaxing, slow summer lifestyle. One night, I got food poisoning and couldn't find !Tylenol to make the pain go away, they insisted I take !aspirin instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vsg2il_BAhmv"
   },
   "source": [
    "# Read in Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "WdCnkG9CBhhT",
    "outputId": "d40e4c47-0291-4de7-fc22-841fb6033ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Writing to:** ../data/train_buffer.tmp\n",
      "**Wrote:** I lived in *Munich last summer. *Germany has a relaxing, slow summer lifestyle. One night, I got food poisoning and couldn't find !Tylenol to make the pain go away, they insisted I take !aspirin instead.\n"
     ]
    }
   ],
   "source": [
    "#@title Training Example\n",
    "\n",
    "TrainingText = \"I lived in *Munich last summer. *Germany has a relaxing, slow summer lifestyle. One night, I got food poisoning and couldn't find !Tylenol to make the pain go away, they insisted I take !aspirin instead.\" #@param {type:\"string\"}\n",
    "read_train(TrainingText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zC3ekCuJAhm3"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "0Lfmoh16Ahm5",
    "outputId": "d80dbdf3-84c9-4f5b-81f2-4e250de2bea7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 920.41it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 3515.76it/s]\n",
      "\n",
      "  0%|          | 0/100000 [00:00<?, ?it/s]\n",
      "100%|##########| 100000/100000 [00:00<00:00, 277641.42it/s]\n",
      "\n",
      "  0%|          | 0/100000 [00:00<?, ?it/s]\n",
      "100%|##########| 100000/100000 [00:00<00:00, 276802.05it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "accuracy: 0.8333, precision 0: 1.0000, precision 1: 0.3333, precision 2: 0.4000, recall 0: 0.8158, recall 1: 1.0000, recall 2: 1.0000, fscore 0: 0.8986, fscore 1: 0.5000, fscore 2: 0.5714, loss: 0.9345 ||: 100%|##########| 1/1 [00:00<00:00, 27.18it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, params, params_copy, extension_pretrained_file = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QyqzzTBoAhnB"
   },
   "source": [
    "# Text Samples for Prediction\n",
    "\n",
    "When I lived in Paris last year, France was experiencing a recession. \n",
    "The night life was too fun, I developed an addiction to Adderall and cocaine.\n",
    "\n",
    "It was a cold, wintery day in Cambridge, England. Michael's grandparents had moved down to Tampa Bay, Florida. \n",
    "The skiiers were on their way to Denver, Colorado for Spring break. \n",
    "I went to my first concert in Las Vegas, Nevada. I hate Brussel sprouts. \n",
    "I think Somerville is one my favorite cities to live it. \n",
    "Computer programming code is developed in Seattle, Washington.\n",
    "<br/>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVBPaTYGAhnC"
   },
   "source": [
    "# Enter Validation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "aCzl9otaAhnD",
    "outputId": "f720836d-2e32-4cf9-83fd-9d4c53edeecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Writing to:** ../data/validate_buffer.tmp\n",
      "**Wrote:** When I lived in Paris last year, France was experiencing a recession. The night life was too fun, I developed an addiction to Adderall and cocaine.\n"
     ]
    }
   ],
   "source": [
    "global VALIDATION_DONE\n",
    "VALIDATION_DONE = False\n",
    "#@title Validation Example\n",
    "\n",
    "ValidationText = \"When I lived in Paris last year, France was experiencing a recession. The night life was too fun, I developed an addiction to Adderall and cocaine.\" #@param {type:\"string\"}\n",
    "read_validation(ValidationText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxp4C3A6AhnJ"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "sLzkGkIuAhnM",
    "outputId": "c6c9744f-2450-4dc0-f7aa-1152fe1a7cd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 748.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4284.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "When I lived in \u001b[31m Paris\u001b[30m last year , \u001b[31m France\u001b[30m was experiencing a recession . The night life was too fun , I developed an addiction to \u001b[34m Adderall\u001b[30m and \u001b[34m cocaine\u001b[30m .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_validation_read(VALIDATION_DONE)\n",
    "edible_params = params_copy.duplicate()\n",
    "out = main(model, params, edible_params, extension_pretrained_file)\n",
    "md_list = generate_markdown(out)\n",
    "md = ' '.join(md_list)\n",
    "print(\"\\n\")\n",
    "printmd(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JybJrF1sAhnU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "instant_colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
