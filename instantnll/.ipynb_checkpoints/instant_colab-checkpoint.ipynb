{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brendanxwhitaker/instantnll/blob/dev/instantnll/instant_colab.ipynb)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/brendanxwhitaker/instantnll/blob/dev/instantnll/instant_colab.ipynb\">direct link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instant Natural Language Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration template. \n",
    "CONFIG_PATH = '../configs/template_colab.jsonnet'\n",
    "\n",
    "# Temporary data buffers. \n",
    "TRAIN_BUFFER_PATH = \"../data/train_buffer.tmp\"\n",
    "VALIDATION_BUFFER_PATH = \"../data/validate_buffer.tmp\"\n",
    "\n",
    "# Check that data has been read. \n",
    "TRAIN_DONE = False\n",
    "VALIDATION_DONE = False\n",
    "\n",
    "# Class highlighting. \n",
    "try:\n",
    "  import colorama\n",
    "except:\n",
    "  !pip install colorama\n",
    "from colorama import Fore, Style\n",
    "CITY_SPAN_OPEN = Fore.RED\n",
    "DRUG_SPAN_OPEN = Fore.BLUE\n",
    "SPAN_CLOSE = Fore.BLACK\n",
    "\n",
    "# Special markdown printing function. \n",
    "def printmd(string: str) -> None:\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### <span style=\"background-color: #54f542\">Imports complete.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/content\")\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "from typing import List\n",
    "\n",
    "try:\n",
    "    from allennlp.common.params import Params\n",
    "    from allennlp.commands.train import train_model\n",
    "    from allennlp.models import Model\n",
    "    from allennlp.data import Vocabulary\n",
    "except:\n",
    "    !pip install allennlp\n",
    "    from allennlp.common.params import Params\n",
    "    from allennlp.commands.train import train_model\n",
    "    from allennlp.models import Model\n",
    "    from allennlp.data import Vocabulary\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from ipywidgets import Layout, widgets\n",
    "\n",
    "try:\n",
    "    from model import InstEntityTagger\n",
    "    from predictor import InstPredictor\n",
    "    from dataset_reader import InstDatasetReader\n",
    "except:\n",
    "    if not os.path.isdir(\"instantnll\"):\n",
    "      !git clone https://github.com/brendanxwhitaker/instantnll.git\n",
    "    os.chdir(\"/content/instantnll/instantnll/\")\n",
    "    from model import InstEntityTagger\n",
    "    from predictor import InstPredictor\n",
    "    from dataset_reader import InstDatasetReader\n",
    "    \n",
    "try:\n",
    "  import wget\n",
    "except:\n",
    "  !pip install wget\n",
    "  import wget\n",
    "\n",
    "printmd(\"Imports complete.\")\n",
    "\n",
    "os.chdir(\"/content/instantnll/data/\")\n",
    "if not os.path.isfile(\"/content/instantnll/data/GoogleNews-vectors-negative300_SUBSET_25000.txt\"):\n",
    "  os.chdir(\"/content/instantnll/data/\")\n",
    "  !wget --no-check-certificate -O \"GoogleNews-vectors-negative300_SUBSET_25000.txt\" \"https://onedrive.live.com/download?cid=5BB216E1C6C27D43&resid=5BB216E1C6C27D43%2118408&authkey=ALcTzCTEipEkqLk\"\n",
    "  os.chdir(\"/content/instantnll/instantnll\")\n",
    "  if not os.path.isfile(\"/content/instantnll/data/GoogleNews-vectors-negative300_SUBSET_25000.txt\"):\n",
    "    raise Exception(\"Couldn't download file.\")\n",
    "\n",
    "def train_prompt() -> None:\n",
    "    train_text = widgets.Text(layout=Layout(width='70%'))\n",
    "    printmd(\"## Training text:\")\n",
    "    display(train_text)\n",
    "    train_text.on_submit(read_train)\n",
    "\n",
    "def validation_prompt() -> None:\n",
    "    valid_text = widgets.Text(layout=Layout(width='70%'))\n",
    "    printmd(\"## Validation text:\")\n",
    "    display(valid_text)\n",
    "    valid_text.on_submit(read_validation)\n",
    "\n",
    "def read_train(sender: widgets.Text) -> None:\n",
    "    global TRAIN_DONE\n",
    "    assert os.path.exists(\"../data/\") # Avoid absolute paths?\n",
    "    # assert not os.path.isfile(TRAIN_BUFFER_PATH)\n",
    "    with open(TRAIN_BUFFER_PATH, 'w') as train_file:\n",
    "        train_file.write(sender.value)\n",
    "    printmd(\"**Writing to:** \" + TRAIN_BUFFER_PATH)\n",
    "    printmd(\"**Wrote:** \" + sender.value)\n",
    "    TRAIN_DONE = True\n",
    "\n",
    "def read_validation(sender: widgets.Text) -> None:\n",
    "    global VALIDATION_DONE\n",
    "    assert os.path.exists(\"../data/\") # Avoid absolute paths?\n",
    "    # assert not os.path.isfile(TRAIN_BUFFER_PATH)\n",
    "    with open(VALIDATION_BUFFER_PATH, 'w') as valid_file:\n",
    "        valid_file.write(sender.value)\n",
    "    printmd(\"**Writing to:** \" + VALIDATION_BUFFER_PATH)\n",
    "    printmd(\"**Wrote:** \" + sender.value)\n",
    "    VALIDATION_DONE = True\n",
    "    \n",
    "def set_params(config_path: str, train_buffer_path: str) -> Params:\n",
    "    # Modifying parameter values\n",
    "    params = Params.from_file(config_path)\n",
    "    params.__setitem__(\"train_data_path\", train_buffer_path)\n",
    "    return params\n",
    "\n",
    "def check_train_read(variable: bool):\n",
    "    try:\n",
    "        assert variable\n",
    "    except AssertionError:\n",
    "        printmd(\"Enter training data above, \\\n",
    "                press <Enter> to submit, then try again.\")\n",
    "        assert variable\n",
    "    \n",
    "def check_validation_read(variable: bool):\n",
    "    try:\n",
    "        assert variable\n",
    "    except AssertionError:\n",
    "        printmd(\"Enter validation data above, \\\n",
    "                press <Enter> to submit, then try again.\")\n",
    "        assert variable\n",
    "\n",
    "def train() -> Model:\n",
    "    check_train_read(TRAIN_DONE)\n",
    "    \n",
    "    # Set parameters. \n",
    "    params = set_params(CONFIG_PATH, TRAIN_BUFFER_PATH)\n",
    "    \n",
    "    # Grab pretrained file.\n",
    "    parms = params.duplicate()\n",
    "    vocab_params = parms.get(key=\"vocabulary\")\n",
    "    pretrained_files_params = vocab_params.get(key=\"pretrained_files\")\n",
    "    extension_pretrained_file = pretrained_files_params.get(key=\"tokens\")\n",
    "    \n",
    "    serialization_dir = tempfile.mkdtemp()\n",
    "    model = train_model(params, serialization_dir)\n",
    "    shutil.rmtree(serialization_dir)\n",
    "    \n",
    "    return model, params, extension_pretrained_file\n",
    "\n",
    "def main(model: Model, params: Params, extension_pretrained_file: str) -> List[List[str]]:\n",
    "    \n",
    "    test_path = VALIDATION_BUFFER_PATH\n",
    "    train_path = TRAIN_BUFFER_PATH\n",
    "\n",
    "    # Get test vocab.\n",
    "    reader = InstDatasetReader()\n",
    "    test_dataset = reader.read(test_path) # Change to temp file.\n",
    "\n",
    "    # Extend vocabulary.\n",
    "    embedding_sources_mapping = {\"word_embeddings.token_embedder_tokens\": extension_pretrained_file}\n",
    "    model.vocab.extend_from_instances(params, test_dataset)\n",
    "    model.extend_embedder_vocab(embedding_sources_mapping)\n",
    "\n",
    "    # Make predictions\n",
    "    predictor = InstPredictor(model, dataset_reader=InstDatasetReader())\n",
    "    with open(test_path, \"r\") as text_file:\n",
    "        lines = text_file.readlines()\n",
    "    all_text = \" \".join(lines) # Makes it all 1 batch.\n",
    "    output_dict = predictor.predict(all_text)\n",
    "    tags = output_dict['tags']\n",
    "    dataset_reader = InstDatasetReader()\n",
    "\n",
    "    PRINT_STDOUT = False\n",
    "    out = []\n",
    "    \n",
    "    with open(\"log.log\", 'a') as log:\n",
    "        for instance in dataset_reader._read(test_path):\n",
    "            tokenlist = list(instance['sentence'])\n",
    "            for i, token in enumerate(tokenlist):\n",
    "                log.write(tags[i] + str(token) + \"\\n\")\n",
    "                if PRINT_STDOUT:\n",
    "                    print(tags[i] + str(token))\n",
    "                out.append([str(token), tags[i]])\n",
    "                \n",
    "    # Allennlp seems to only support extending the vocabulary once.\n",
    "    # This is a hack to modify the `old` number of embeddings at each iteration.\n",
    "    extended_num_embeddings = len(model.vocab.get_index_to_token_vocabulary(namespace='tokens'))\n",
    "    model.word_embeddings.token_embedder_tokens.num_embeddings = extended_num_embeddings\n",
    "    \n",
    "    return out\n",
    "\n",
    "def generate_markdown(out: List[List[str]]) -> List[str]:\n",
    "    md_list = []\n",
    "    for pair in out:\n",
    "        if pair[1] == '*': # Cities\n",
    "            md_list.append(CITY_SPAN_OPEN)\n",
    "            md_list.append(pair[0] + SPAN_CLOSE)\n",
    "        elif pair[1] == '!': # Drugs\n",
    "            md_list.append(DRUG_SPAN_OPEN)\n",
    "            md_list.append(pair[0] + SPAN_CLOSE)\n",
    "        else:\n",
    "            md_list.append(pair[0])\n",
    "    return md_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Samples for Training\n",
    "\n",
    "I lived in *Munich last summer. *Germany has a relaxing, slow summer lifestyle. One night, I got food poisoning and couldn't find !Tylenol to make the pain go away, they insisted I take !aspirin instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Training text:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7040226217442b6a77a4c328e4805f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='70%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Writing to:** ../data/train_buffer.tmp"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Wrote:** I lived in *Munich last summer. *Germany has a relaxing, slow summer lifestyle. One night, I got food poisoning and couldn't find !Tylenol to make the pain go away, they insisted I take !aspirin instead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 162.25it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 1885.93it/s]\n",
      "\n",
      "  0%|          | 0/500000 [00:00<?, ?it/s]\n",
      "100%|##########| 500000/500000 [00:06<00:00, 75369.12it/s]\n",
      "\n",
      "  0%|          | 0/500000 [00:00<?, ?it/s]\n",
      "100%|##########| 500000/500000 [00:06<00:00, 78310.29it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "accuracy: 0.8333, precision 0: 1.0000, precision 1: 0.3333, precision 2: 0.4000, recall 0: 0.8158, recall 1: 1.0000, recall 2: 1.0000, fscore 0: 0.8986, fscore 1: 0.5000, fscore 2: 0.5714, loss: 0.9315 ||: 100%|##########| 1/1 [00:00<00:00, 18.79it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, params, extension_pretrained_file = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Samples for Prediction\n",
    "\n",
    "When I lived in Paris last year, France was experiencing a recession. \n",
    "The night life was too fun, I developed an addiction to Adderall and cocaine.\n",
    "\n",
    "It was a cold, wintery day in Cambridge, England. Michael's grandparents had moved down to Tampa Bay, Florida. \n",
    "The skiiers were on their way to Denver, Colorado for Spring break. \n",
    "I went to my first concert in Las Vegas, Nevada. I hate Brussel sprouts. \n",
    "I think Somerville is one my favorite cities to live it. \n",
    "Computer programming code is developed in Seattle, Washington.\n",
    "<br/>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter Validation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Validation text:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3d1839c302464f850b4879b7da7422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='70%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Writing to:** ../data/validate_buffer.tmp"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Wrote:** When I lived in Paris last year, France was experiencing a recession. The night life was too fun, I developed an addiction to Adderall and cocaine."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global VALIDATION_DONE\n",
    "VALIDATION_DONE = False\n",
    "validation_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 311.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1056.23it/s]\n",
      "100%|██████████| 500000/500000 [00:06<00:00, 77799.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "When I lived in <span style=\"background-color: #ff5050\"> Paris</span> last year , <span style=\"background-color: #ff5050\"> France</span> was experiencing a recession . The night life was too fun , I developed an addiction to <span style=\"background-color: #3399ff\"> Adderall</span> and <span style=\"background-color: #3399ff\"> cocaine</span> ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_validation_read(VALIDATION_DONE)\n",
    "out = main(model, params, extension_pretrained_file)\n",
    "md_list = generate_markdown(out)\n",
    "md = ' '.join(md_list)\n",
    "printmd(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
